{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b78b07-9f47-43f0-95bb-41a0df3a2b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1914056 entries, 0 to 1914055\n",
      "Data columns (total 45 columns):\n",
      " #   Column                           Dtype  \n",
      "---  ------                           -----  \n",
      " 0   Unique ID                        int64  \n",
      " 1   Rider_ID                         int64  \n",
      " 2   category_x                       object \n",
      " 3   Circuit_Length_km                float64\n",
      " 4   Laps                             int64  \n",
      " 5   Grid_Position                    int64  \n",
      " 6   Avg_Speed_kmh                    float64\n",
      " 7   Track_Condition                  object \n",
      " 8   Humidity_%                       int64  \n",
      " 9   Tire_Compound_Front              object \n",
      " 10  Tire_Compound_Rear               object \n",
      " 11  Penalty                          object \n",
      " 12  Championship_Points              int64  \n",
      " 13  Championship_Position            int64  \n",
      " 14  Session                          object \n",
      " 15  year_x                           int64  \n",
      " 16  sequence                         int64  \n",
      " 17  rider                            int64  \n",
      " 18  team                             int64  \n",
      " 19  bike                             int64  \n",
      " 20  position                         int64  \n",
      " 21  points                           float64\n",
      " 22  shortname                        object \n",
      " 23  circuit_name                     object \n",
      " 24  rider_name                       object \n",
      " 25  team_name                        object \n",
      " 26  bike_name                        object \n",
      " 27  Lap_Time_Seconds                 float64\n",
      " 28  Corners_per_Lap                  int64  \n",
      " 29  Tire_Degradation_Factor_per_Lap  float64\n",
      " 30  Pit_Stop_Duration_Seconds        float64\n",
      " 31  Ambient_Temperature_Celsius      float64\n",
      " 32  Track_Temperature_Celsius        float64\n",
      " 33  weather                          object \n",
      " 34  track                            object \n",
      " 35  air                              int64  \n",
      " 36  ground                           int64  \n",
      " 37  starts                           int64  \n",
      " 38  finishes                         int64  \n",
      " 39  with_points                      int64  \n",
      " 40  podiums                          int64  \n",
      " 41  wins                             int64  \n",
      " 42  min_year                         int64  \n",
      " 43  max_year                         int64  \n",
      " 44  years_active                     int64  \n",
      "dtypes: float64(8), int64(24), object(13)\n",
      "memory usage: 657.1+ MB\n",
      "None\n",
      "          Unique ID      Rider_ID  Circuit_Length_km          Laps  \\\n",
      "count  1.914056e+06  1.914056e+06       1.914056e+06  1.914056e+06   \n",
      "mean   1.367777e+06  5.509394e+03       4.701208e+00  2.150189e+01   \n",
      "std    7.890630e+05  2.597186e+03       6.910968e-01  2.293772e+00   \n",
      "min    0.000000e+00  1.000000e+03       3.500000e+00  1.800000e+01   \n",
      "25%    6.847348e+05  3.258000e+03       4.104000e+00  1.900000e+01   \n",
      "50%    1.368276e+06  5.514000e+03       4.702000e+00  2.200000e+01   \n",
      "75%    2.050815e+06  7.760000e+03       5.299000e+00  2.400000e+01   \n",
      "max    2.734366e+06  9.998000e+03       5.900000e+00  2.500000e+01   \n",
      "\n",
      "       Grid_Position  Avg_Speed_kmh    Humidity_%  Championship_Points  \\\n",
      "count   1.914056e+06   1.914056e+06  1.914056e+06         1.914056e+06   \n",
      "mean    1.149819e+01   2.496328e+02  5.951643e+01         1.744615e+02   \n",
      "std     6.334418e+00   5.773525e+01  1.733707e+01         1.009485e+02   \n",
      "min     1.000000e+00   1.500000e+02  3.000000e+01         0.000000e+00   \n",
      "25%     6.000000e+00   1.993500e+02  4.400000e+01         8.700000e+01   \n",
      "50%     1.200000e+01   2.496500e+02  6.000000e+01         1.740000e+02   \n",
      "75%     1.700000e+01   2.994600e+02  7.500000e+01         2.620000e+02   \n",
      "max     2.200000e+01   3.500000e+02  8.900000e+01         3.490000e+02   \n",
      "\n",
      "       Championship_Position        year_x  ...           air        ground  \\\n",
      "count           1.914056e+06  1.914056e+06  ...  1.914056e+06  1.914056e+06   \n",
      "mean            1.254921e+01  1.997913e+03  ...  2.190190e+01  2.973457e+01   \n",
      "std             6.913828e+00  1.708566e+01  ...  5.928275e+00  1.162753e+01   \n",
      "min             1.000000e+00  1.949000e+03  ...  1.200000e+01  1.200000e+01   \n",
      "25%             7.000000e+00  1.987000e+03  ...  1.700000e+01  2.100000e+01   \n",
      "50%             1.300000e+01  2.001000e+03  ...  2.100000e+01  2.900000e+01   \n",
      "75%             1.900000e+01  2.012000e+03  ...  2.600000e+01  4.000000e+01   \n",
      "max             2.400000e+01  2.021000e+03  ...  3.600000e+01  5.400000e+01   \n",
      "\n",
      "             starts      finishes   with_points       podiums          wins  \\\n",
      "count  1.914056e+06  1.914056e+06  1.914056e+06  1.914056e+06  1.914056e+06   \n",
      "mean   1.012372e+02  8.746167e+01  7.096455e+01  1.399721e+01  7.374099e+00   \n",
      "std    8.143524e+01  6.986620e+01  6.685675e+01  2.615523e+01  1.641622e+01   \n",
      "min    1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    3.500000e+01  3.100000e+01  1.700000e+01  0.000000e+00  0.000000e+00   \n",
      "50%    8.500000e+01  7.400000e+01  5.300000e+01  4.000000e+00  1.000000e+00   \n",
      "75%    1.460000e+02  1.230000e+02  1.060000e+02  1.600000e+01  7.000000e+00   \n",
      "max    4.060000e+02  3.730000e+02  3.650000e+02  1.780000e+02  1.180000e+02   \n",
      "\n",
      "           min_year      max_year  years_active  \n",
      "count  1.914056e+06  1.914056e+06  1.914056e+06  \n",
      "mean   1.993047e+03  2.002509e+03  9.377671e+00  \n",
      "std    1.683261e+01  1.747976e+01  5.144729e+00  \n",
      "min    1.949000e+03  1.949000e+03  1.000000e+00  \n",
      "25%    1.983000e+03  1.991000e+03  5.000000e+00  \n",
      "50%    1.996000e+03  2.007000e+03  9.000000e+00  \n",
      "75%    2.006000e+03  2.019000e+03  1.200000e+01  \n",
      "max    2.021000e+03  2.021000e+03  2.600000e+01  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "val = pd.read_csv('val.csv')\n",
    "print(train.info())\n",
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7f0959-73b2-40ee-83a1-c58506141358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ID                               0\n",
      "Rider_ID                                0\n",
      "category_x                              0\n",
      "Circuit_Length_km                       0\n",
      "Laps                                    0\n",
      "Grid_Position                           0\n",
      "Avg_Speed_kmh                           0\n",
      "Track_Condition                         0\n",
      "Humidity_%                              0\n",
      "Tire_Compound_Front                     0\n",
      "Tire_Compound_Rear                      0\n",
      "Penalty                            321292\n",
      "Championship_Points                     0\n",
      "Championship_Position                   0\n",
      "Session                                 0\n",
      "year_x                                  0\n",
      "sequence                                0\n",
      "rider                                   0\n",
      "team                                    0\n",
      "bike                                    0\n",
      "position                                0\n",
      "points                                  0\n",
      "shortname                               0\n",
      "circuit_name                            0\n",
      "rider_name                              0\n",
      "team_name                               0\n",
      "bike_name                               0\n",
      "Lap_Time_Seconds                        0\n",
      "Corners_per_Lap                         0\n",
      "Tire_Degradation_Factor_per_Lap         0\n",
      "Pit_Stop_Duration_Seconds               0\n",
      "Ambient_Temperature_Celsius             0\n",
      "Track_Temperature_Celsius               0\n",
      "weather                                 0\n",
      "track                                   0\n",
      "air                                     0\n",
      "ground                                  0\n",
      "starts                                  0\n",
      "finishes                                0\n",
      "with_points                             0\n",
      "podiums                                 0\n",
      "wins                                    0\n",
      "min_year                                0\n",
      "max_year                                0\n",
      "years_active                            0\n",
      "dtype: int64\n",
      "           Unique ID      Rider_ID category_x  Circuit_Length_km  \\\n",
      "count   1.914056e+06  1.914056e+06    1914056       1.914056e+06   \n",
      "unique           NaN           NaN          3                NaN   \n",
      "top              NaN           NaN      Moto2                NaN   \n",
      "freq             NaN           NaN     640761                NaN   \n",
      "mean    1.367777e+06  5.509394e+03        NaN       4.701208e+00   \n",
      "std     7.890630e+05  2.597186e+03        NaN       6.910968e-01   \n",
      "min     0.000000e+00  1.000000e+03        NaN       3.500000e+00   \n",
      "25%     6.847348e+05  3.258000e+03        NaN       4.104000e+00   \n",
      "50%     1.368276e+06  5.514000e+03        NaN       4.702000e+00   \n",
      "75%     2.050815e+06  7.760000e+03        NaN       5.299000e+00   \n",
      "max     2.734366e+06  9.998000e+03        NaN       5.900000e+00   \n",
      "\n",
      "                Laps  Grid_Position  Avg_Speed_kmh Track_Condition  \\\n",
      "count   1.914056e+06   1.914056e+06   1.914056e+06         1914056   \n",
      "unique           NaN            NaN            NaN               2   \n",
      "top              NaN            NaN            NaN             Wet   \n",
      "freq             NaN            NaN            NaN          959552   \n",
      "mean    2.150189e+01   1.149819e+01   2.496328e+02             NaN   \n",
      "std     2.293772e+00   6.334418e+00   5.773525e+01             NaN   \n",
      "min     1.800000e+01   1.000000e+00   1.500000e+02             NaN   \n",
      "25%     1.900000e+01   6.000000e+00   1.993500e+02             NaN   \n",
      "50%     2.200000e+01   1.200000e+01   2.496500e+02             NaN   \n",
      "75%     2.400000e+01   1.700000e+01   2.994600e+02             NaN   \n",
      "max     2.500000e+01   2.200000e+01   3.500000e+02             NaN   \n",
      "\n",
      "          Humidity_% Tire_Compound_Front  ...           air        ground  \\\n",
      "count   1.914056e+06             1914056  ...  1.914056e+06  1.914056e+06   \n",
      "unique           NaN                   3  ...           NaN           NaN   \n",
      "top              NaN              Medium  ...           NaN           NaN   \n",
      "freq             NaN              639333  ...           NaN           NaN   \n",
      "mean    5.951643e+01                 NaN  ...  2.190190e+01  2.973457e+01   \n",
      "std     1.733707e+01                 NaN  ...  5.928275e+00  1.162753e+01   \n",
      "min     3.000000e+01                 NaN  ...  1.200000e+01  1.200000e+01   \n",
      "25%     4.400000e+01                 NaN  ...  1.700000e+01  2.100000e+01   \n",
      "50%     6.000000e+01                 NaN  ...  2.100000e+01  2.900000e+01   \n",
      "75%     7.500000e+01                 NaN  ...  2.600000e+01  4.000000e+01   \n",
      "max     8.900000e+01                 NaN  ...  3.600000e+01  5.400000e+01   \n",
      "\n",
      "              starts      finishes   with_points       podiums          wins  \\\n",
      "count   1.914056e+06  1.914056e+06  1.914056e+06  1.914056e+06  1.914056e+06   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean    1.012372e+02  8.746167e+01  7.096455e+01  1.399721e+01  7.374099e+00   \n",
      "std     8.143524e+01  6.986620e+01  6.685675e+01  2.615523e+01  1.641622e+01   \n",
      "min     1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%     3.500000e+01  3.100000e+01  1.700000e+01  0.000000e+00  0.000000e+00   \n",
      "50%     8.500000e+01  7.400000e+01  5.300000e+01  4.000000e+00  1.000000e+00   \n",
      "75%     1.460000e+02  1.230000e+02  1.060000e+02  1.600000e+01  7.000000e+00   \n",
      "max     4.060000e+02  3.730000e+02  3.650000e+02  1.780000e+02  1.180000e+02   \n",
      "\n",
      "            min_year      max_year  years_active  \n",
      "count   1.914056e+06  1.914056e+06  1.914056e+06  \n",
      "unique           NaN           NaN           NaN  \n",
      "top              NaN           NaN           NaN  \n",
      "freq             NaN           NaN           NaN  \n",
      "mean    1.993047e+03  2.002509e+03  9.377671e+00  \n",
      "std     1.683261e+01  1.747976e+01  5.144729e+00  \n",
      "min     1.949000e+03  1.949000e+03  1.000000e+00  \n",
      "25%     1.983000e+03  1.991000e+03  5.000000e+00  \n",
      "50%     1.996000e+03  2.007000e+03  9.000000e+00  \n",
      "75%     2.006000e+03  2.019000e+03  1.200000e+01  \n",
      "max     2.021000e+03  2.021000e+03  2.600000e+01  \n",
      "\n",
      "[11 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(train.isnull().sum())\n",
    "\n",
    "# Get descriptive statistics\n",
    "print(train.describe(include='all'))  # Include all columns for a complete overview\n",
    "# Fill missing values in the 'Penalty' column\n",
    "train['Penalty'] = train['Penalty'].fillna('No Penalty')  # or use 0 if it's numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2bb79f-ed60-4e22-bf99-d05eab85459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame after feature creation:\n",
      "   Unique ID  Rider_ID category_x  Circuit_Length_km  Laps  Grid_Position  \\\n",
      "0    1894944      2659      Moto2              4.874    22             17   \n",
      "1      23438      5205      Moto2              3.875    24              7   \n",
      "2     939678      7392      Moto3              5.647    25              5   \n",
      "3    1196312      7894      Moto3              4.810    19              3   \n",
      "4    1033899      6163     MotoGP              5.809    25             21   \n",
      "\n",
      "   Avg_Speed_kmh Track_Condition  Humidity_% Tire_Compound_Front  ... podiums  \\\n",
      "0         264.66             Wet          61                Hard  ...       4   \n",
      "1         177.56             Wet          77                Soft  ...       2   \n",
      "2         317.74             Dry          87                Soft  ...       0   \n",
      "3         321.82             Wet          43                Soft  ...      16   \n",
      "4         239.92             Wet          47                Hard  ...      29   \n",
      "\n",
      "  wins  min_year  max_year years_active  Lap_Density  Grid_Championship_Diff  \\\n",
      "0    0      2018      2021            4     4.513746                      -3   \n",
      "1    1      1975      1983            8     6.193548                       0   \n",
      "2    0      1982      1989            8     4.427129                     -12   \n",
      "3    9      1994      2009           16     3.950104                     -10   \n",
      "4   17      2011      2021           11     4.303667                      13   \n",
      "\n",
      "   Total_Tire_Wear  Win_Rate  Podium_Rate  \n",
      "0           0.0924  0.000000     0.075472  \n",
      "1           0.0600  0.037037     0.074074  \n",
      "2           0.1100  0.000000     0.000000  \n",
      "3           0.0646  0.046875     0.083333  \n",
      "4           0.0425  0.097143     0.165714  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Validation DataFrame after feature creation:\n",
      "   Unique ID  Rider_ID category_x  Circuit_Length_km  Laps  Grid_Position  \\\n",
      "0    1733872      6065      Moto2              4.248    24              5   \n",
      "1    1778161      5781      Moto2              4.907    20              2   \n",
      "2    1205004      3780      Moto2              4.162    24             15   \n",
      "3     578473      8193     MotoGP              3.889    18              3   \n",
      "4    2248803      1632      Moto2              4.088    24             19   \n",
      "\n",
      "   Avg_Speed_kmh Track_Condition  Humidity_% Tire_Compound_Front  ... podiums  \\\n",
      "0         293.12             Wet          33                Hard  ...      15   \n",
      "1         310.31             Wet          32                Soft  ...       4   \n",
      "2         237.39             Wet          89              Medium  ...     105   \n",
      "3         275.55             Wet          32                Soft  ...       5   \n",
      "4         297.81             Wet          59                Soft  ...       1   \n",
      "\n",
      "  wins  min_year  max_year years_active  Lap_Density  Grid_Championship_Diff  \\\n",
      "0    6      1966      1970            5     5.649718                      -8   \n",
      "1    1      1996      2005           10     4.075810                      -7   \n",
      "2   54      2001      2021           19     5.766458                      10   \n",
      "3    2      1998      2010           13     4.628439                      -4   \n",
      "4    0      2010      2018            7     5.870841                      -4   \n",
      "\n",
      "   Total_Tire_Wear  Win_Rate  Podium_Rate  \n",
      "0           0.0648  0.111111     0.277778  \n",
      "1           0.0620  0.008333     0.033333  \n",
      "2           0.0864  0.186207     0.362069  \n",
      "3           0.0612  0.014925     0.037313  \n",
      "4           0.0504  0.000000     0.009434  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "\n",
      "Test DataFrame after feature creation:\n",
      "   Unique ID  Rider_ID category_x  Circuit_Length_km  Laps  Grid_Position  \\\n",
      "0     288307      6533     MotoGP              5.316    19              1   \n",
      "1     704288      4800      Moto2              3.861    19              2   \n",
      "2     951491      7907      Moto3              3.954    18             21   \n",
      "3    2591721      4844      Moto3              4.074    21             21   \n",
      "4    1202653      4802      Moto2              4.096    25             21   \n",
      "\n",
      "   Avg_Speed_kmh Track_Condition  Humidity_% Tire_Compound_Front  ... podiums  \\\n",
      "0         284.38             Dry          44                Hard  ...       2   \n",
      "1         328.98             Dry          43                Hard  ...     178   \n",
      "2         165.56             Dry          77                Soft  ...      13   \n",
      "3         258.46             Dry          74              Medium  ...       0   \n",
      "4         319.60             Dry          84                Hard  ...       5   \n",
      "\n",
      "  wins  min_year  max_year years_active  Lap_Density  Grid_Championship_Diff  \\\n",
      "0    0      1981      1993           13     3.574116                     -17   \n",
      "1  111      1996      2021           26     4.921005                      -8   \n",
      "2    7      1993      2007           15     4.552352                      19   \n",
      "3    0      1998      2007           10     5.154639                       9   \n",
      "4    2      1998      2010           13     6.103516                      18   \n",
      "\n",
      "   Total_Tire_Wear  Win_Rate  Podium_Rate  \n",
      "0           0.0874  0.000000     0.021739  \n",
      "1           0.0437  0.273399     0.438424  \n",
      "2           0.0198  0.050725     0.094203  \n",
      "3           0.0609  0.000000     0.000000  \n",
      "4           0.0825  0.014925     0.037313  \n",
      "\n",
      "[5 rows x 49 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the function to create new features\n",
    "import numpy as np\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Example features\n",
    "    df['Lap_Density'] = df['Laps'] / df['Circuit_Length_km'].replace(0, np.nan)  # Avoid division by zero\n",
    "    df['Grid_Championship_Diff'] = df['Grid_Position'] - df['Championship_Position']\n",
    "    df['Total_Tire_Wear'] = df['Tire_Degradation_Factor_per_Lap'] * df['Laps']\n",
    "    \n",
    "    # Use the correct column names\n",
    "    df['Win_Rate'] = df['wins'] / df['starts'].replace(0, np.nan)  # Corrected to 'starts'\n",
    "    df['Podium_Rate'] = df['podiums'] / df['starts'].replace(0, np.nan)  # Corrected to 'starts'\n",
    "    \n",
    "    # Replace infinite values with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature creation\n",
    "train = create_features(train)\n",
    "val = create_features(val)\n",
    "test = create_features(test)\n",
    "# Display the first few rows of the modified DataFrame\n",
    "print(\"Train DataFrame after feature creation:\")\n",
    "print(train.head())\n",
    "print(\"\\nValidation DataFrame after feature creation:\")\n",
    "print(val.head())\n",
    "print(\"\\nTest DataFrame after feature creation:\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "696afa05-e8a5-42ac-9cb4-137dacdfc588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this cell is run BEFORE using train_encoded\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode_categoricals(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # One-hot encoding for low cardinality categorical features\n",
    "    one_hot_cols = [\n",
    "        'category_x', 'Track_Condition', 'Tire_Compound_Front', \n",
    "        'Tire_Compound_Rear', 'Session', 'weather'\n",
    "    ]\n",
    "    df = pd.get_dummies(df, columns=one_hot_cols, drop_first=True)\n",
    "    \n",
    "    # Label encoding for high cardinality categorical features\n",
    "    label_cols = ['circuit_name', 'rider_name', 'team_name', 'bike_name']\n",
    "    le = LabelEncoder()\n",
    "    for col in label_cols:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_encoded = encode_categoricals(train)\n",
    "val_encoded = encode_categoricals(val)\n",
    "test_encoded = encode_categoricals(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc9d84-84d9-49d9-9fef-3d5d2bd76b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "X_train shape: (1914056, 60)\n",
      "y_train shape: (1914056,)\n",
      "X_val shape: (273437, 60)\n",
      "y_val shape: (273437,)\n",
      "\n",
      "Data types in X_train:\n",
      "int64      29\n",
      "bool       17\n",
      "float64    11\n",
      "object      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Object columns found in features: ['Penalty', 'shortname', 'track']\n",
      "Sample values in Penalty: ['+3s' '+5s' 'DNF' 'DNS' 'Ride Through' 'No Penalty']\n",
      "Sample values in shortname: ['EMI' 'NAT' 'SPA' 'AUS' 'CAT' 'VAL' 'MAL' 'RSA' 'JPN' 'WGER']\n",
      "Sample values in track: ['Dry' 'Wet']\n",
      "\n",
      "Cleaning and encoding feature matrices...\n",
      "Processing categorical column: Penalty\n",
      "Unique values: ['+3s' '+5s' 'DNF' 'DNS' 'Ride Through' 'No Penalty']\n",
      "Encoded Penalty with penalty values\n",
      "Processing categorical column: shortname\n",
      "Unique values: ['EMI' 'NAT' 'SPA' 'AUS' 'CAT' 'VAL' 'MAL' 'RSA' 'JPN' 'WGER' 'FRA' 'NED'\n",
      " 'EGER' 'AME' 'GBR' 'RIO' 'ARG' 'ITA' 'TCH' 'CZE' 'EUR' 'AUT' 'RSM' 'SWE'\n",
      " 'BEL' 'GER' 'POR' 'JUG' 'INP' 'CHN' 'ULST' 'VDU' 'FIN' 'BRA' 'QAT' 'ARA'\n",
      " 'USA' 'INA' 'TT' 'VEN' 'ANC' 'STY' 'IMO' 'TUR' 'HUN' 'THA' 'PAC' 'DOH'\n",
      " 'MAD' 'TER' 'SWI' 'FIM' 'CAN']\n",
      "Label encoded column: shortname\n",
      "Processing categorical column: track\n",
      "Unique values: ['Dry' 'Wet']\n",
      "Label encoded column: track\n",
      "Processing categorical column: Penalty\n",
      "Unique values: [nan 'DNS' '+5s' '+3s' 'DNF' 'Ride Through']\n",
      "Found unmapped penalty values, using label encoding for: [nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7j/v2yhh4y152gg0cpxq7q_lv_h0000gn/T/ipykernel_23419/4190860459.py:45: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[nan nan nan ... nan nan nan]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_cleaned.loc[unmapped_mask, col] = df.loc[unmapped_mask, col]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Penalty with penalty values\n",
      "Processing categorical column: shortname\n",
      "Unique values: ['EGER' 'POR' 'JPN' 'NED' 'CZE' 'GBR' 'QAT' 'CAT' 'FRA' 'EUR' 'ITA' 'WGER'\n",
      " 'FIN' 'SPA' 'VAL' 'RSM' 'ARG' 'BEL' 'USA' 'MAL' 'JUG' 'STY' 'SWE' 'RSA'\n",
      " 'AUS' 'AUT' 'TCH' 'NAT' 'ARA' 'INP' 'GER' 'ULST' 'CAN' 'HUN' 'RIO' 'TUR'\n",
      " 'PAC' 'EMI' 'TT' 'IMO' 'AME' 'CHN' 'BRA' 'VEN' 'MAD' 'SWI' 'TER' 'INA'\n",
      " 'FIM' 'DOH' 'THA' 'ANC' 'VDU']\n",
      "Label encoded column: shortname\n",
      "Processing categorical column: track\n",
      "Unique values: ['Dry' 'Wet']\n",
      "Label encoded column: track\n",
      "\n",
      "Checking target variable...\n",
      "No non-numeric values found in target variable\n",
      "\n",
      "Final data type check:\n",
      "X_train_cleaned dtypes:\n",
      "int64      32\n",
      "bool       17\n",
      "float64    11\n",
      "Name: count, dtype: int64\n",
      "✓ All feature columns are now numeric!\n",
      "\n",
      "NaN values in X_train_cleaned: 0\n",
      "NaN values in y_train_cleaned: 0\n",
      "\n",
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Function to clean and encode categorical data\n",
    "def clean_and_encode_data(df):\n",
    "    \"\"\"Clean dataframe by encoding categorical variables and converting numeric strings\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in df_cleaned.columns:\n",
    "        if df_cleaned[col].dtype == 'object':\n",
    "            print(f\"Processing categorical column: {col}\")\n",
    "            print(f\"Unique values: {df_cleaned[col].unique()}\")\n",
    "            \n",
    "            # Special handling for Penalty column (contains time penalties)\n",
    "            if col == 'Penalty':\n",
    "                # Create a numeric representation of penalties\n",
    "                penalty_mapping = {\n",
    "                    'No Penalty': 0,\n",
    "                    '+3s': 3,\n",
    "                    '+5s': 5,\n",
    "                    '+10s': 10,\n",
    "                    '+15s': 15,\n",
    "                    '+20s': 20,\n",
    "                    '+30s': 30,\n",
    "                    'Ride Through': 25,  # Approximate time penalty\n",
    "                    'DNF': 999,  # Large penalty for Did Not Finish\n",
    "                    'DNS': 998,  # Large penalty for Did Not Start\n",
    "                    'DSQ': 997   # Large penalty for Disqualified\n",
    "                }\n",
    "                \n",
    "                # Apply mapping, use label encoding for unmapped values\n",
    "                df_cleaned[col] = df_cleaned[col].map(penalty_mapping)\n",
    "                unmapped_mask = df_cleaned[col].isna()\n",
    "                \n",
    "                if unmapped_mask.any():\n",
    "                    print(f\"Found unmapped penalty values, using label encoding for: {df[col][unmapped_mask].unique()}\")\n",
    "                    le = LabelEncoder()\n",
    "                    # Fill unmapped values with original strings temporarily\n",
    "                    df_cleaned.loc[unmapped_mask, col] = df.loc[unmapped_mask, col]\n",
    "                    # Encode all values\n",
    "                    df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))\n",
    "                    label_encoders[col] = le\n",
    "                \n",
    "                print(f\"Encoded {col} with penalty values\")\n",
    "            \n",
    "            else:\n",
    "                # Use label encoding for other categorical columns\n",
    "                le = LabelEncoder()\n",
    "                df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "                print(f\"Label encoded column: {col}\")\n",
    "    \n",
    "    return df_cleaned, label_encoders\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = train_encoded.drop(columns=['Lap_Time_Seconds'])\n",
    "y_train = train_encoded['Lap_Time_Seconds']\n",
    "X_val = val_encoded.drop(columns=['Lap_Time_Seconds'])\n",
    "y_val = val_encoded['Lap_Time_Seconds']\n",
    "\n",
    "# Check shapes to confirm\n",
    "print(\"Original shapes:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "\n",
    "# Check data types in X_train\n",
    "print(\"\\nData types in X_train:\")\n",
    "print(X_train.dtypes.value_counts())\n",
    "\n",
    "# Check for non-numeric columns in features\n",
    "object_cols = X_train.select_dtypes(include=['object']).columns\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"\\nObject columns found in features: {list(object_cols)}\")\n",
    "    \n",
    "    # Show sample values from object columns\n",
    "    for col in object_cols:\n",
    "        print(f\"Sample values in {col}: {X_train[col].unique()[:10]}\")\n",
    "\n",
    "# Clean and encode the feature matrices\n",
    "print(\"\\nCleaning and encoding feature matrices...\")\n",
    "X_train_cleaned, train_encoders = clean_and_encode_data(X_train)\n",
    "X_val_cleaned, val_encoders = clean_and_encode_data(X_val)\n",
    "\n",
    "# Function to clean the target variable\n",
    "def clean_target(y):\n",
    "    \"\"\"Clean target variable by removing unwanted characters and converting to float\"\"\"\n",
    "    return y.astype(str).str.replace('+', '').str.replace('s', '').astype(float)\n",
    "\n",
    "# Check for non-numeric values in y_train\n",
    "print(\"\\nChecking target variable...\")\n",
    "non_numeric_mask = ~pd.to_numeric(y_train, errors='coerce').notna()\n",
    "non_numeric_values = y_train[non_numeric_mask]\n",
    "\n",
    "# Print any non-numeric values found\n",
    "if not non_numeric_values.empty:\n",
    "    print(\"Non-numeric values in y_train:\")\n",
    "    print(non_numeric_values.head(10))\n",
    "    print(f\"Total non-numeric values: {len(non_numeric_values)}\")\n",
    "    \n",
    "    # Clean the target variables\n",
    "    y_train_cleaned = clean_target(y_train)\n",
    "    y_val_cleaned = clean_target(y_val)\n",
    "else:\n",
    "    print(\"No non-numeric values found in target variable\")\n",
    "    # If no cleaning needed, use original target\n",
    "    y_train_cleaned = y_train.astype(float)\n",
    "    y_val_cleaned = y_val.astype(float)\n",
    "\n",
    "# Final check - ensure all data is numeric\n",
    "print(\"\\nFinal data type check:\")\n",
    "print(\"X_train_cleaned dtypes:\")\n",
    "print(X_train_cleaned.dtypes.value_counts())\n",
    "\n",
    "# Verify all columns are now numeric\n",
    "non_numeric_features = X_train_cleaned.select_dtypes(include=['object']).columns\n",
    "if len(non_numeric_features) > 0:\n",
    "    print(f\"\\nError: Still have non-numeric columns: {list(non_numeric_features)}\")\n",
    "    # Convert any remaining object columns to numeric if possible\n",
    "    for col in non_numeric_features:\n",
    "        try:\n",
    "            X_train_cleaned[col] = pd.to_numeric(X_train_cleaned[col], errors='coerce')\n",
    "            X_val_cleaned[col] = pd.to_numeric(X_val_cleaned[col], errors='coerce')\n",
    "            print(f\"Converted {col} to numeric\")\n",
    "        except:\n",
    "            print(f\"Could not convert {col} to numeric - this will cause an error\")\n",
    "else:\n",
    "    print(\"✓ All feature columns are now numeric!\")\n",
    "\n",
    "# Verify no NaN values were introduced\n",
    "print(f\"\\nNaN values in X_train_cleaned: {X_train_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"NaN values in y_train_cleaned: {y_train_cleaned.isnull().sum()}\")\n",
    "\n",
    "# Handle any NaN values if they exist\n",
    "if X_train_cleaned.isnull().sum().sum() > 0:\n",
    "    print(\"Filling NaN values with median...\")\n",
    "    X_train_cleaned = X_train_cleaned.fillna(X_train_cleaned.median())\n",
    "    X_val_cleaned = X_val_cleaned.fillna(X_train_cleaned.median())  # Use training median for validation\n",
    "\n",
    "if y_train_cleaned.isnull().sum() > 0:\n",
    "    print(\"Removing rows with NaN target values...\")\n",
    "    valid_idx = ~y_train_cleaned.isnull()\n",
    "    X_train_cleaned = X_train_cleaned[valid_idx]\n",
    "    y_train_cleaned = y_train_cleaned[valid_idx]\n",
    "\n",
    "# Initialize model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train model using the cleaned data\n",
    "print(\"\\nTraining model...\")\n",
    "model.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val_cleaned)\n",
    "\n",
    "# Calculate RMSE using the cleaned validation target\n",
    "rmse = mean_squared_error(y_val_cleaned, y_pred, squared=False)\n",
    "print(f\"\\nValidation RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Additional metrics\n",
    "mae = np.mean(np.abs(y_val_cleaned - y_pred))\n",
    "print(f\"Validation MAE: {mae:.4f}\")\n",
    "\n",
    "# Show some predictions vs actual\n",
    "print(f\"\\nSample predictions vs actual:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_val_cleaned.iloc[:10].values,\n",
    "    'Predicted': y_pred[:10],\n",
    "    'Difference': (y_val_cleaned.iloc[:10].values - y_pred[:10])\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2d2371-7cbd-4999-81e7-04fcd1ba3258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MotoGP Lap Time Prediction Pipeline ===\n",
      "Train shape: (1914056, 45)\n",
      "Val shape: (273437, 45)\n",
      "\n",
      "Running hyperparameter tuning...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best hyperparameters: {'min_samples_leaf': 10, 'max_iter': 500, 'max_depth': 7, 'learning_rate': 0.05, 'l2_regularization': 0.1}\n",
      "Validation RMSE: 10.7929\n",
      "\n",
      "Saving predictions to submission1.csv...\n",
      "✅ submission1.csv created successfully.\n",
      "\n",
      "=== Pipeline completed successfully! ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== MotoGP Lap Time Prediction Pipeline ===\")\n",
    "\n",
    "# Step 1: Load data\n",
    "train = pd.read_csv('train.csv')\n",
    "val = pd.read_csv('val.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Val shape: {val.shape}\")\n",
    "\n",
    "# Use a subset of training data for faster hyperparameter tuning\n",
    "train_sample = train.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    if 'Penalty' in df.columns:\n",
    "        df['Penalty'] = df['Penalty'].fillna(\"No Penalty\")\n",
    "    df['Lap_Density'] = df['Laps'] / df['Circuit_Length_km'].replace(0, np.nan)\n",
    "    df['Grid_Championship_Diff'] = df['Grid_Position'] - df['Championship_Position']\n",
    "    df['Total_Tire_Wear'] = df['Tire_Degradation_Factor_per_Lap'] * df['Laps']\n",
    "    df['Win_Rate'] = df['wins'] / df['starts'].replace(0, np.nan)\n",
    "    df['Podium_Rate'] = df['podiums'] / df['starts'].replace(0, np.nan)\n",
    "    df['Temp_Diff'] = df['Track_Temperature_Celsius'] - df['Ambient_Temperature_Celsius']\n",
    "    df['Humidity_Temp'] = df['Humidity_%'] * df['Ambient_Temperature_Celsius']\n",
    "    df['Is_Race_Session'] = df['Session'].str.lower().eq('race').astype(int)\n",
    "    df['Career_Progress'] = df['year_x'] - df['min_year']\n",
    "    df['Weather_TrackCond'] = df['weather'].astype(str) + '_' + df['Track_Condition'].astype(str)\n",
    "    return df\n",
    "\n",
    "train_sample = create_features(train_sample)\n",
    "val = create_features(val)\n",
    "\n",
    "# Step 3: Encode categorical features\n",
    "def encode_data(train_df, val_df):\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    one_hot_cols = ['category_x', 'Track_Condition', 'Tire_Compound_Front',\n",
    "                    'Tire_Compound_Rear', 'Session', 'weather']\n",
    "    label_cols = ['circuit_name', 'rider_name', 'team_name', 'bike_name', 'Penalty', 'Weather_TrackCond']\n",
    "\n",
    "    # Remove high-cardinality object columns that weren't properly encoded\n",
    "    drop_cols = ['track', 'shortname']\n",
    "    for col in drop_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df.drop(columns=[col], inplace=True)\n",
    "        if col in val_df.columns:\n",
    "            val_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    combined = pd.concat([train_df[one_hot_cols], val_df[one_hot_cols]])\n",
    "    combined_encoded = pd.get_dummies(combined, drop_first=True)\n",
    "    train_dummies = combined_encoded.iloc[:len(train_df)]\n",
    "    val_dummies = combined_encoded.iloc[len(train_df):]\n",
    "    train_df = train_df.drop(columns=one_hot_cols).reset_index(drop=True)\n",
    "    val_df = val_df.drop(columns=one_hot_cols).reset_index(drop=True)\n",
    "    train_df = pd.concat([train_df, train_dummies.reset_index(drop=True)], axis=1)\n",
    "    val_df = pd.concat([val_df, val_dummies.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    for col in label_cols:\n",
    "        if col in train_df.columns:\n",
    "            le = LabelEncoder()\n",
    "            all_vals = pd.concat([train_df[col].astype(str), val_df[col].astype(str)])\n",
    "            le.fit(all_vals)\n",
    "            train_df[col] = le.transform(train_df[col].astype(str))\n",
    "            val_df[col] = le.transform(val_df[col].astype(str))\n",
    "    return train_df, val_df\n",
    "\n",
    "train_sample, val_encoded = encode_data(train_sample, val)\n",
    "\n",
    "# Step 4: Fill missing values\n",
    "def fill_missing(df, ref=None):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            fill_val = ref[col].median() if ref is not None and col in ref.columns else df[col].median()\n",
    "            df[col] = df[col].fillna(fill_val)\n",
    "        else:\n",
    "            df[col] = df[col].fillna(\"Unknown\")\n",
    "    return df\n",
    "\n",
    "train_sample = fill_missing(train_sample)\n",
    "val_encoded = fill_missing(val_encoded, train_sample)\n",
    "\n",
    "# Step 5: Prepare features and target\n",
    "target = 'Lap_Time_Seconds'\n",
    "X_train_sample = train_sample.drop(columns=[target])\n",
    "y_train_sample = train_sample[target]\n",
    "X_val = val_encoded.drop(columns=[target])\n",
    "y_val = val_encoded[target]\n",
    "\n",
    "common_features = sorted(list(set(X_train_sample.columns).intersection(set(X_val.columns))))\n",
    "X_train_sample = X_train_sample[common_features]\n",
    "X_val = X_val[common_features]\n",
    "\n",
    "# Step 6: Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'max_iter': [100, 300, 500],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'min_samples_leaf': [10, 20, 50],\n",
    "    'l2_regularization': [0.0, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "print(\"\\nRunning hyperparameter tuning...\")\n",
    "base_model = HistGradientBoostingRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Step 7: Train model with best parameters\n",
    "model = random_search.best_estimator_\n",
    "model.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Step 8: Evaluate\n",
    "y_val_pred = model.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Step 9: Save submission\n",
    "print(\"\\nSaving predictions to submission1.csv...\")\n",
    "if 'Unique ID' in val.columns:\n",
    "    submission = pd.DataFrame({\n",
    "        'Unique ID': val['Unique ID'],\n",
    "        'Lap_Time_Seconds': y_val_pred\n",
    "    })\n",
    "    submission.to_csv(\"submission1.csv\", index=False)\n",
    "    print(\"✅ submission1.csv created successfully.\")\n",
    "else:\n",
    "    print(\"❌ 'Unique ID' column not found in validation set.\")\n",
    "\n",
    "print(\"\\n=== Pipeline completed successfully! ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26744165-6bc2-43c7-9b1a-0892915c88c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unique ID', 'Rider_ID', 'category_x', 'Circuit_Length_km', 'Laps',\n",
      "       'Grid_Position', 'Avg_Speed_kmh', 'Track_Condition', 'Humidity_%',\n",
      "       'Tire_Compound_Front', 'Tire_Compound_Rear', 'Penalty',\n",
      "       'Championship_Points', 'Championship_Position', 'Session', 'year_x',\n",
      "       'sequence', 'rider', 'team', 'bike', 'position', 'points', 'shortname',\n",
      "       'circuit_name', 'rider_name', 'team_name', 'bike_name',\n",
      "       'Lap_Time_Seconds', 'Corners_per_Lap',\n",
      "       'Tire_Degradation_Factor_per_Lap', 'Pit_Stop_Duration_Seconds',\n",
      "       'Ambient_Temperature_Celsius', 'Track_Temperature_Celsius', 'weather',\n",
      "       'track', 'air', 'ground', 'starts', 'finishes', 'with_points',\n",
      "       'podiums', 'wins', 'min_year', 'max_year', 'years_active',\n",
      "       'Lap_Density', 'Grid_Championship_Diff', 'Total_Tire_Wear', 'Win_Rate',\n",
      "       'Podium_Rate', 'Temp_Diff', 'Humidity_Temp', 'Is_Race_Session',\n",
      "       'Career_Progress', 'Weather_TrackCond'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(val.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e30ce72-b69d-491f-a067-0a4a435a80db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in train.csv:\n",
      "['Unique ID', 'Rider_ID', 'category_x', 'Circuit_Length_km', 'Laps', 'Grid_Position', 'Avg_Speed_kmh', 'Track_Condition', 'Humidity_%', 'Tire_Compound_Front', 'Tire_Compound_Rear', 'Penalty', 'Championship_Points', 'Championship_Position', 'Session', 'year_x', 'sequence', 'rider', 'team', 'bike', 'position', 'points', 'shortname', 'circuit_name', 'rider_name', 'team_name', 'bike_name', 'Lap_Time_Seconds', 'Corners_per_Lap', 'Tire_Degradation_Factor_per_Lap', 'Pit_Stop_Duration_Seconds', 'Ambient_Temperature_Celsius', 'Track_Temperature_Celsius', 'weather', 'track', 'air', 'ground', 'starts', 'finishes', 'with_points', 'podiums', 'wins', 'min_year', 'max_year', 'years_active']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train dataset\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Print the names of all columns\n",
    "print(\"Columns in train.csv:\")\n",
    "print(train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72665912-85e9-4a11-a292-f799b18189eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MotoGP Lap Time Prediction Pipeline ===\n",
      "Train shape: (1914056, 45)\n",
      "Val shape: (273437, 45)\n",
      "\n",
      "Running hyperparameter tuning...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "Best hyperparameters: {'min_samples_leaf': 5, 'max_iter': 500, 'max_depth': 15, 'learning_rate': 0.05, 'l2_regularization': 1.0}\n",
      "Validation RMSE: 10.7354\n",
      "\n",
      "Saving predictions to submission1.csv...\n",
      "✅ submission1.csv created successfully.\n",
      "\n",
      "=== Pipeline completed successfully! ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== MotoGP Lap Time Prediction Pipeline ===\")\n",
    "\n",
    "# Step 1: Load data\n",
    "train = pd.read_csv('train.csv')\n",
    "val = pd.read_csv('val.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Val shape: {val.shape}\")\n",
    "\n",
    "# Use a subset of training data for faster hyperparameter tuning\n",
    "train_sample = train.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    if 'Penalty' in df.columns:\n",
    "        df['Penalty'] = df['Penalty'].fillna(\"No Penalty\")\n",
    "    df['Lap_Density'] = df['Laps'] / df['Circuit_Length_km'].replace(0, np.nan)\n",
    "    df['Grid_Championship_Diff'] = df['Grid_Position'] - df['Championship_Position']\n",
    "    df['Total_Tire_Wear'] = df['Tire_Degradation_Factor_per_Lap'] * df['Laps']\n",
    "    df['Win_Rate'] = df['wins'] / df['starts'].replace(0, np.nan)\n",
    "    df['Podium_Rate'] = df['podiums'] / df['starts'].replace(0, np.nan)\n",
    "    df['Temp_Diff'] = df['Track_Temperature_Celsius'] - df['Ambient_Temperature_Celsius']\n",
    "    df['Humidity_Temp'] = df['Humidity_%'] * df['Ambient_Temperature_Celsius']\n",
    "    df['Is_Race_Session'] = df['Session'].str.lower().eq('race').astype(int)\n",
    "    df['Career_Progress'] = df['year_x'] - df['min_year']\n",
    "    df['Weather_TrackCond'] = df['weather'].astype(str) + '_' + df['Track_Condition'].astype(str)\n",
    "    return df\n",
    "\n",
    "train_sample = create_features(train_sample)\n",
    "val = create_features(val)\n",
    "\n",
    "# Step 3: Encode categorical features\n",
    "def encode_data(train_df, val_df):\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    one_hot_cols = ['category_x', 'Track_Condition', 'Tire_Compound_Front',\n",
    "                    'Tire_Compound_Rear', 'Session', 'weather']\n",
    "    label_cols = ['circuit_name', 'rider_name', 'team_name', 'bike_name', 'Penalty', 'Weather_TrackCond']\n",
    "\n",
    "    # Remove high-cardinality object columns that weren't properly encoded\n",
    "    drop_cols = ['track', 'shortname']\n",
    "    for col in drop_cols:\n",
    "        if col in train_df.columns:\n",
    "            train_df.drop(columns=[col], inplace=True)\n",
    "        if col in val_df.columns:\n",
    "            val_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    combined = pd.concat([train_df[one_hot_cols], val_df[one_hot_cols]])\n",
    "    combined_encoded = pd.get_dummies(combined, drop_first=True)\n",
    "    train_dummies = combined_encoded.iloc[:len(train_df)]\n",
    "    val_dummies = combined_encoded.iloc[len(train_df):]\n",
    "    train_df = train_df.drop(columns=one_hot_cols).reset_index(drop=True)\n",
    "    val_df = val_df.drop(columns=one_hot_cols).reset_index(drop=True)\n",
    "    train_df = pd.concat([train_df, train_dummies.reset_index(drop=True)], axis=1)\n",
    "    val_df = pd.concat([val_df, val_dummies.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    for col in label_cols:\n",
    "        if col in train_df.columns:\n",
    "            le = LabelEncoder()\n",
    "            all_vals = pd.concat([train_df[col].astype(str), val_df[col].astype(str)])\n",
    "            le.fit(all_vals)\n",
    "            train_df[col] = le.transform(train_df[col].astype(str))\n",
    "            val_df[col] = le.transform(val_df[col].astype(str))\n",
    "    return train_df, val_df\n",
    "\n",
    "train_sample, val_encoded = encode_data(train_sample, val)\n",
    "\n",
    "# Step 4: Fill missing values\n",
    "def fill_missing(df, ref=None):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            fill_val = ref[col].median() if ref is not None and col in ref.columns else df[col].median()\n",
    "            df[col] = df[col].fillna(fill_val)\n",
    "        else:\n",
    "            df[col] = df[col].fillna(\"Unknown\")\n",
    "    return df\n",
    "\n",
    "train_sample = fill_missing(train_sample)\n",
    "val_encoded = fill_missing(val_encoded, train_sample)\n",
    "\n",
    "# Step 5: Prepare features and target\n",
    "target = 'Lap_Time_Seconds'\n",
    "X_train_sample = train_sample.drop(columns=[target])\n",
    "y_train_sample = train_sample[target]\n",
    "X_val = val_encoded.drop(columns=[target])\n",
    "y_val = val_encoded[target]\n",
    "\n",
    "common_features = sorted(list(set(X_train_sample.columns).intersection(set(X_val.columns))))\n",
    "X_train_sample = X_train_sample[common_features]\n",
    "X_val = X_val[common_features]\n",
    "\n",
    "# Step 6: Hyperparameter Tuning with broader search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.02, 0.03, 0.05],\n",
    "    'max_iter': [300, 500, 700],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'l2_regularization': [0.0, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "print(\"\\nRunning hyperparameter tuning...\")\n",
    "base_model = HistGradientBoostingRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=25,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Step 7: Train model with best parameters\n",
    "model = random_search.best_estimator_\n",
    "model.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Step 8: Evaluate\n",
    "y_val_pred = model.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Step 9: Save submission\n",
    "print(\"\\nSaving predictions to submission1.csv...\")\n",
    "if 'Unique ID' in val.columns:\n",
    "    submission = pd.DataFrame({\n",
    "        'Unique ID': val['Unique ID'],\n",
    "        'Lap_Time_Seconds': y_val_pred\n",
    "    })\n",
    "    submission.to_csv(\"submission1.csv\", index=False)\n",
    "    print(\"✅ submission1.csv created successfully.\")\n",
    "else:\n",
    "    print(\"❌ 'Unique ID' column not found in validation set.\")\n",
    "\n",
    "print(\"\\n=== Pipeline completed successfully! ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec51028-2979-4f6e-a998-779f3104b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Random Forest parameters with larger folds...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load your datasets\n",
    "train = pd.read_csv('train.csv')  # Update path as needed\n",
    "val = pd.read_csv('val.csv')\n",
    "\n",
    "# Step 2: Feature engineering function\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['Lap_Density'] = df['Laps'] / df['Circuit_Length_km'].replace(0, np.nan)\n",
    "    df['Grid_Championship_Diff'] = df['Grid_Position'] - df['Championship_Position']\n",
    "    df['Total_Tire_Wear'] = df['Tire_Degradation_Factor_per_Lap'] * df['Laps']\n",
    "    df['Win_Rate'] = df['wins'] / df['starts'].replace(0, np.nan)\n",
    "    df['Podium_Rate'] = df['podiums'] / df['starts'].replace(0, np.nan)\n",
    "    df['Pit_Per_Lap'] = df['Pit_Stop_Duration_Seconds'] / df['Laps'].replace(0, np.nan)\n",
    "    df['Temp_Diff'] = df['Track_Temperature_Celsius'] - df['Ambient_Temperature_Celsius']\n",
    "    df['Humidity_Temp'] = df['Humidity_%'] * df['Ambient_Temperature_Celsius']\n",
    "    df['Is_Race_Session'] = df['Session'].str.lower().eq('race').astype(int)\n",
    "    df['Career_Progress'] = df['year_x'] - df['min_year']\n",
    "    df['Weather_TrackCond'] = df['weather'].astype(str) + '_' + df['Track_Condition'].astype(str)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "train = create_features(train)\n",
    "val = create_features(val)\n",
    "\n",
    "# Step 3: Encoding categorical variables\n",
    "def encode_categoricals(df):\n",
    "    df = df.copy()\n",
    "    one_hot_cols = ['category_x', 'Track_Condition', 'Tire_Compound_Front', 'Tire_Compound_Rear', 'Session', 'weather', 'Weather_TrackCond']\n",
    "    df = pd.get_dummies(df, columns=one_hot_cols, drop_first=True)\n",
    "    label_cols = ['circuit_name', 'rider_name', 'team_name', 'bike_name', 'Penalty', 'shortname', 'track']\n",
    "    for col in label_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "train_encoded = encode_categoricals(train)\n",
    "val_encoded = encode_categoricals(val)\n",
    "\n",
    "# Step 4: Impute missing values\n",
    "def fill_missing(df):\n",
    "    df = df.copy()\n",
    "    num_cols = df.select_dtypes(include='number').columns\n",
    "    for col in num_cols:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    for col in cat_cols:\n",
    "        df[col].fillna('Unknown', inplace=True)\n",
    "    return df\n",
    "\n",
    "train_encoded = fill_missing(train_encoded)\n",
    "val_encoded = fill_missing(val_encoded)\n",
    "\n",
    "# Step 5: Prepare features and target\n",
    "X_train = train_encoded.drop(columns=['Lap_Time_Seconds'])\n",
    "y_train = train_encoded['Lap_Time_Seconds']\n",
    "X_val = val_encoded.drop(columns=['Lap_Time_Seconds'])\n",
    "y_val = val_encoded['Lap_Time_Seconds']\n",
    "\n",
    "# Step 6: Tune hyperparameters for Random Forest using GridSearchCV with larger folds\n",
    "print(\"Tuning Random Forest parameters with larger folds...\")\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [20, 40, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=1)  # Increased cv to 5\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(\"Best RF params:\", grid_rf.best_params_)\n",
    "\n",
    "# Step 7: Stacking ensemble with best models\n",
    "stack = StackingRegressor(\n",
    "    estimators=[('rf', best_rf)],\n",
    "    final_estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "print(\"Training stacking ensemble model...\")\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate on the validation dataset\n",
    "y_val_pred = stack.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "print(f\"Stacking Ensemble Validation RMSE: {rmse:.4f} seconds\")\n",
    "\n",
    "# Step 9: Feature importance analysis (based on Random Forest)\n",
    "importances = best_rf.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Top 20 Feature Importances (Random Forest)\")\n",
    "plt.bar(range(20), importances[indices][:20], align=\"center\")\n",
    "plt.xticks(range(20), [features[i] for i in indices][:20], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Drop least important features (e.g. features with importance < threshold) and retrain\n",
    "importance_threshold = 0.005  # Adjust based on plot observation\n",
    "important_features = features[importances > importance_threshold]\n",
    "\n",
    "print(f\"Reducing feature set from {len(features)} to {len(important_features)} features...\")\n",
    "\n",
    "X_train_reduced = X_train[important_features]\n",
    "X_val_reduced = X_val[important_features]\n",
    "\n",
    "# Retrain on reduced feature set\n",
    "stack.fit(X_train_reduced, y_train)\n",
    "y_val_pred_reduced = stack.predict(X_val_reduced)\n",
    "rmse_reduced = mean_squared_error(y_val, y_val_pred_reduced, squared=False)\n",
    "print(f\"Validation RMSE after feature pruning: {rmse_reduced:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038634e-eeb4-470c-ac35-dc5a450f8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Load the datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')  # Load the training dataset\n",
    "    val_df = pd.read_csv('val.csv')      # Load the validation dataset\n",
    "    test_df = pd.read_csv('test.csv')    # Load the test dataset\n",
    "    sample_submission = pd.read_csv('sample_submission.csv')  # Load the sample submission file\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 2: Prepare the training data\n",
    "X_train = train_df.drop(columns=['Lap_Time_Seconds'], errors='ignore')  # Replace with your target column name\n",
    "y_train = train_df['Lap_Time_Seconds']  # Replace with your target column name\n",
    "\n",
    "# Step 3: Prepare the validation data\n",
    "X_val = val_df.drop(columns=['Lap_Time_Seconds'], errors='ignore')  # Replace with your target column name\n",
    "y_val = val_df['Lap_Time_Seconds']  # Replace with your target column name\n",
    "\n",
    "# Step 4: Encode the training and validation data\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_val_encoded = pd.get_dummies(X_val, drop_first=True)\n",
    "\n",
    "# Align the columns of the validation set with the training set\n",
    "X_val_encoded = X_val_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Step 5: Train the XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Step 6: Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val_encoded)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "print(\"Root Mean Squared Error (RMSE) on validation set:\", rmse)\n",
    "\n",
    "# Step 8: Prepare the test data\n",
    "X_test = test_df.drop(columns=['Unique ID'], errors='ignore')  # Drop any non-feature columns\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Align the columns of the test set with the training set\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Step 9: Make predictions on the test set\n",
    "test_predictions = model.predict(X_test_encoded)\n",
    "\n",
    "# Step 10: Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'Unique ID': test_df['Unique ID'],  # Include the Unique ID from the test set\n",
    "    'Lap_Time_Seconds': test_predictions  # Include the predictions\n",
    "})\n",
    "\n",
    "# Step 11: Save the submission file as submission3.csv\n",
    "submission_df.to_csv('submission3.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission3.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf4d3f-ee91-4547-8d28-8f0b8a3542d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37d5a58-cc0b-496e-82fc-8e8688bb6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c1f7ab-875a-4cc5-a427-0448b6f20080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "print(\"✅ Files loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf12685-d3a9-4b35-8d0c-15defa4e2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=['Lap_Time_Seconds'], errors='ignore')\n",
    "y_train = train_df['Lap_Time_Seconds']\n",
    "\n",
    "X_val = val_df.drop(columns=['Lap_Time_Seconds'], errors='ignore')\n",
    "y_val = val_df['Lap_Time_Seconds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db6f5bb-c5bd-44ff-bdf5-e41eacfba2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Detect categorical columns (object types)\n",
    "cat_cols = X_train.select_dtypes(include='object').columns\n",
    "\n",
    "# Use OrdinalEncoder (faster, efficient for large datasets)\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_train[cat_cols] = encoder.fit_tansform(X_train[cat_cols])\n",
    "X_val[cat_cols] = encoder.transform(X_val[cat_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054bf3e-0014-4bc2-8918-7ab01246a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "print(\"✅ Files loaded successfully.\")\n",
    "# Step 2: Prepare the training data\n",
    "X_train = train_df.drop(columns=['Lap_Time_Seconds'], errors='ignore')  # Replace with your target column name\n",
    "y_train = train_df['Lap_Time_Seconds']  # Replace with your target column name\n",
    "\n",
    "# Step 3: Prepare the validation data\n",
    "X_val = val_df.drop(columns=['Lap_Time_Seconds'], errors='ignore')  # Replace with your target column name\n",
    "y_val = val_df['Lap_Time_Seconds']  # Replace with your target column name\n",
    "\n",
    "# Step 4: Encode the training and validation data\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_val_encoded = pd.get_dummies(X_val, drop_first=True)\n",
    "\n",
    "# Align the columns of the validation set with the training set\n",
    "X_val_encoded = X_val_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Step 5: Train the XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Step 6: Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e89a85-a5b5-4a6e-ae21-53e87e37cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.405529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4046\n",
      "[LightGBM] [Info] Number of data points in the train set: 1914056, number of used features: 42\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n",
      "[LightGBM] [Info] Start training from score 4.502737\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's rmse: 0.0562225\tvalid_0's l2: 0.00316097\n",
      "[200]\tvalid_0's rmse: 0.0266165\tvalid_0's l2: 0.000708436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tvalid_0's rmse: 0.013014\tvalid_0's l2: 0.000169365\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load CSVs\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "\n",
    "# Target column\n",
    "target = \"Lap_Time_Seconds\"\n",
    "\n",
    "# --- Drop NA only from training data ---\n",
    "train_df = train_df.dropna(subset=[target])\n",
    "\n",
    "# Combine for consistent preprocessing\n",
    "df = pd.concat([train_df, val_df], axis=0, ignore_index=True)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "df['is_penalized'] = (df['Penalty'].notna() & (df['Penalty'] != 'None')).astype(int)\n",
    "df['speed_per_km'] = df['Avg_Speed_kmh'] / df['Circuit_Length_km']\n",
    "df['lap_time_per_km'] = df[target] / df['Circuit_Length_km']\n",
    "df['laps_squared'] = df['Laps'] ** 2\n",
    "df['temp_diff'] = df['Track_Temperature_Celsius'] - df['Ambient_Temperature_Celsius']\n",
    "df['humidity_ratio'] = df['Humidity_%'] / 100\n",
    "df['combined_temp'] = df['Track_Temperature_Celsius'] + df['Ambient_Temperature_Celsius']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "drop_cols = ['Unique ID', 'rider_name', 'shortname', 'team_name', 'bike_name', 'rider', 'team', 'bike', 'Penalty']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Encode categorical columns\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "df[cat_cols] = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "# Log-transform the target column\n",
    "df[target] = np.log1p(df[target])\n",
    "\n",
    "# Split the combined DataFrame back into train and validation sets\n",
    "train_rows = train_df.shape[0]\n",
    "train_data = df.iloc[:train_rows]\n",
    "val_data = df.iloc[train_rows:]\n",
    "\n",
    "X_train = train_data.drop(columns=[target])\n",
    "y_train = train_data[target]\n",
    "X_val = val_data.drop(columns=[target])\n",
    "y_val = val_data[target]  # may contain NaN — won't be used for training\n",
    "\n",
    "# Train LightGBM model\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=8,\n",
    "    num_leaves=64,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.5,\n",
    "    min_gain_to_split=0.001,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training LightGBM...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val[~y_val.isna()], y_val[~y_val.isna()])],  # only use non-NaN rows for validation\n",
    "    eval_metric=\"rmse\",\n",
    "    callbacks=[early_stopping(100), log_evaluation(100)]\n",
    ")\n",
    "\n",
    "# Predict on all val data\n",
    "y_pred_log = model.predict(X_val)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# Evaluate RMSE only on known validation targets\n",
    "y_val_true = np.expm1(y_val[~y_val.isna()])\n",
    "y_val_pred_known = y_pred[~y_val.isna()]\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_val_true, y_val_pred_known))\n",
    "print(f\"\\n✅ Final Validation RMSE (on known targets): {rmse:.4f}\")\n",
    "\n",
    "# --- Prepare submission with exactly 546874 rows ---\n",
    "assert val_df.shape[0] == 546874, f\"Expected 546874 rows, but got {val_df.shape[0]}\"\n",
    "\n",
    "submission = val_df[['Unique ID']].copy()\n",
    "submission['Lap_Time_Seconds'] = y_pred\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv(\"submission2.csv\", index=False)\n",
    "print(f\"\\n📄 Submission file 'submission2.csv' has been created with {submission.shape[0]} rows.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
